# Ollama GPU-Accelerated Stack
# Standalone deployment for Ollama with GPU acceleration
# Environment variables are loaded from .env file by default
#
# PREREQUISITES:
# - AMD ROCm or NVIDIA drivers installed on host
# - External networks: ollama_network and openwebui_stack
# - See README.md for detailed setup instructions
#
# GPU ACCELERATION NOTES:
# - Supports both NVIDIA and AMD GPU acceleration
# - AMD GPU acceleration uses ROCm runtime
# - NVIDIA GPU acceleration uses CUDA runtime
# - Device access requires host system GPU drivers

version: "3.8"

services:
  ollama:
    image: "ollama/ollama:${OLLAMA_IMAGE_TAG:-rocm}"
    container_name: "${CONTAINER_NAME_PREFIX:-ollama}_ollama"
    environment:
      - TZ=${TZ:-UTC}
      
      # AMD GPU (ROCm) Configuration
      - ROCR_VISIBLE_DEVICES=${ROCR_VISIBLE_DEVICES:-all}
      - HSA_OVERRIDE_GFX_VERSION_0=${HSA_OVERRIDE_GFX_VERSION_0:-11.0.0}
      - HSA_OVERRIDE_GFX_VERSION_1=${HSA_OVERRIDE_GFX_VERSION_1:-11.0.0}
      
      # NVIDIA GPU Configuration (uncomment if using NVIDIA)
      # - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      # - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      
      # Ollama Configuration
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
      - OLLAMA_PORT=${OLLAMA_PORT:-11434}
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-1}
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-2}
      - OLLAMA_DEBUG=${OLLAMA_DEBUG:-false}
      
    volumes:
      # Bind mount to specified path for Ollama data and models
      - "/mnt/docker_vol/openweb_ui/ollama:/root/.ollama"
      
      # GPU device access for AMD
      - "/dev/kfd:/dev/kfd"
      - "/dev/dri:/dev/dri"
      
    devices:
      # AMD GPU devices
      - "/dev/kfd"
      - "/dev/dri"
      
    # AMD GPU runtime
    runtime: amd
    
    networks:
      - ollama_network
      - openwebui_stack
    
    ports:
      - "${OLLAMA_HOST_PORT:-11434}:11434"
    

    deploy:
      replicas: 1
      resources:
        limits:
          memory: ${OLLAMA_MEMORY_LIMIT:-8G}
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      labels:
        - "amd_gpu=true"
        - "ollama.service=true"
        - "traefik.enable=false"  # Disable Traefik for this service
    
    logging:
      driver: json-file
      options:
        max-file: '5'
        max-size: 10m

networks:
  # Internal network for Ollama
  ollama_network:
    name: "${OLLAMA_NETWORK_NAME:-ollama_network}"
    external: true
  
  # External network to connect with Open-WebUI stack
  openwebui_stack:
    name: "${OPENWEBUI_STACK_NETWORK:-openwebui_stack}"
    external: true
